# **Assignment 4 â€“ Descriptive Statistics & Power Transformation (Diabetes Dataset)**

## ðŸ“Œ **Google Colab Link:**
ðŸ”— [https://colab.research.google.com/drive/1j93CM43YCyz88cQr7ahY34B4KnQbJpu_?usp=sharing](https://colab.research.google.com/drive/1j93CM43YCyz88cQr7ahY34B4KnQbJpu_?usp=sharing)

---

## ðŸ“˜ **1. Introduction**

This assignment focuses on applying **descriptive statistical analysis** and **power transformation** to a real regression dataset, followed by evaluation using **Linear Regression**. The primary goal is to understand how data transformation affects model performance and distribution characteristics.

The practice section uses the *Iris dataset* to build familiarity with descriptive statistics. The main assignment uses the *Diabetes dataset*, which is a standard regression dataset available in scikit-learn.

---

## ðŸ§  **2. Dataset Description**

### Practice Dataset

* **Iris Dataset**

  * Contains 150 samples of three flower species.
  * Used for practice only (classification dataset).

### Main Dataset

* **Diabetes Dataset**

  * A regression dataset containing clinical measurements and a target disease progression metric.
  * Fully numerical, suitable for regression and transformation.

---

## ðŸ›  **3. Steps Performed**

### ðŸ”¹ Step 1 â€” Import Libraries

We imported essential Python libraries for:

* Data handling (`pandas`, `numpy`)
* Visualization (`matplotlib`, `seaborn`)
* Machine learning (`scikit-learn`)
* Statistical analysis (`scipy.stats`)

---

### ðŸ”¹ Step 2 â€” Load Dataset

The Diabetes dataset was loaded using scikit-learnâ€™s `load_diabetes()` method and converted into a Pandas DataFrame, including the target column.

---

### ðŸ”¹ Step 3 â€” Handle Missing Values

Checked for null entries using `.isnull().sum()` and confirmed that the dataset contains no missing values.

---

### ðŸ”¹ Step 4 â€” Descriptive Statistics

Used `.describe()` to compute:

* Mean
* Standard deviation
* Minimum, maximum
* Quartiles

This helps understand general data behavior before modeling.

---

### ðŸ”¹ Step 5 â€” Train-Test Split

Split features and target into:

* **Training set (67%)**
* **Testing set (33%)**

This is standard practice to evaluate model performance on unseen data.

---

### ðŸ”¹ Step 6 â€” Linear Regression (Before Transformation)

Performed a baseline Linear Regression:

```python
lr = LinearRegression()
lr.fit(X_train, y_train)
```

Calculated RÂ² score to measure initial model performance:

```
RÂ² before transformation: 0.5192
```

---

### ðŸ”¹ Step 7 â€” Check Data Distribution

Plotted:

* Histogram
* Probability plot (Q-Q plot)

for each feature to understand normality and skewness.

---

### ðŸ”¹ Step 8 â€” Power Transformation

Applied **Yeo-Johnson** transformation:

```python
pt = PowerTransformer(method='yeo-johnson')
```

This method was chosen because:

* It handles both positive and negative values.
* It improves normality of features.
* It is more flexible than Box-Cox in real regression datasets.

---

### ðŸ”¹ Step 9 â€” Linear Regression (After Transformation)

Trained the model again using transformed data and calculated new performance:

```
RÂ² after transformation: 0.5049
```

---

### ðŸ”¹ Step 10 â€” Compare Before & After

Plotted distributions before and after transformation to confirm improvement in data normality visually.

---

## ðŸ“Š **4. Results**

| Metric                         | Value      |
| ------------------------------ | ---------- |
| RÂ² before transformation       | **0.5192** |
| RÂ² after Yeo-Johnson transform | **0.5049** |

---

## ðŸ“Œ **5. Interpretation and Conclusion**

* The **baseline Linear Regression** showed moderate performance with an RÂ² of ~0.52.
* After applying **Yeo-Johnson power transformation**, the model performance **slightly decreased** (RÂ² ~0.50).
* This outcome shows that **power transformation does not always improve predictive performance**, but it does help make feature distributions more normal.
* Since the dataset has **zero and negative values**, Yeo-Johnson was the appropriate transformation method rather than Box-Cox.
* Visual distribution plots before and after transformation confirm improved symmetry and reduced skewness in the features.

---

## ðŸ§¾ **6. Key Learnings**

1. **Descriptive statistics** provide a deep understanding of feature behavior.
2. **Power transformations** such as Yeo-Johnson help improve feature normality.
3. **Model evaluation metrics** (like RÂ²) help compare model performance objectively.
4. Real-world transformation may not always increase accuracy but often makes model assumptions more valid.
5. It is important to choose the correct **power transformation** method based on data characteristics.

---

## ðŸŽ“ **7. Conclusion**

> This assignment demonstrated the full workflow of regression analysis with transformation. After exploring descriptive statistics and applying Yeo-Johnson transformation, it was concluded that while transformation improved data normality, it did not enhance model performance in this case. However, it remains an important tool for satisfying mathematical assumptions of regression models.

---
---

<div align="center"> <h1 style=font-weight: bold;>@PSCodersHub</h1> </div>